# Persian Speech Emotion Recognition (SER)

## ğŸ”¥ Overview
Understanding emotions in speech is crucial for natural human-computer interaction. However, accurately capturing emotionsâ€”especially in low-resource languages like Persianâ€”poses significant challenges. This project introduces a **multimodal Speech Emotion Recognition (SER) system** that integrates **acoustic and textual features** to enhance emotion classification accuracy.

### ğŸ¯ Key Contributions
- **Multimodal Fusion:** Combines **acoustic** and **textual** features for improved recognition.
- **Whisper ASR:** Converts speech to text for extracting linguistic features.
- **Modified Differential Evolution (MDE):** Optimized feature selection technique.
- **Self-Attention Mechanism:** Enhances the fusion of extracted features.
- **Deep Learning-based Classification:** Leverages CNN and Transformer-based architectures.

## ğŸ“Œ Dataset
We evaluate our model on the **ShEMO (Sharif Speech Emotion) dataset**, a well-established Persian speech emotion corpus.

## ğŸš€ Installation & Usage
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/YOUR_GITHUB_USERNAME/Persian-SER.git
cd Persian-SER
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the Model
```bash
python main.py --mode train  # Train the model
python main.py --mode test   # Test the model
```

## ğŸ“Š Model Architecture
### ğŸ”¹ Feature Extraction
- **Acoustic Features:** MFCCs, spectral descriptors, and LLDs
- **Text Features:** Speech-to-text conversion using Whisper ASR, followed by tokenization and embedding

### ğŸ”¹ Feature Selection
- **Modified Differential Evolution (MDE)** optimizes feature selection, reducing dimensionality while improving classification accuracy.

### ğŸ”¹ Classification Model
- **CNN-1D** for acoustic feature extraction.
- **CNN-2D** for textual feature processing.
- **Self-Attention Mechanism** to enhance multimodal fusion.
- **Final Emotion Classification** via a deep learning-based model.

## ğŸ“ˆ Results
| Model | Accuracy |
|--------|---------|
| Baseline Acoustic Model | 74.5% |
| Baseline Text Model | 76.8% |
| **Proposed Multimodal Model** | **82.3%** |

## ğŸ› ï¸ Requirements
- Python 3.7+
- PyTorch / TensorFlow
- Hugging Face Transformers
- Whisper ASR

## ğŸ“œ Citation
If you use this project, please cite our paper:
```
@article{your_paper,
  author    = {Your Name et al.},
  title     = {Multimodal Speech Emotion Recognition with Evolutionary Optimization},
  journal   = {Your Conference/Journal},
  year      = {2024}
}
```

## ğŸ“¬ Contact
For questions or collaborations, reach out via [your email] or open an issue on GitHub!

---
ğŸ¯ **Keywords:** Speech Processing, Emotion Recognition, Differential Evolution, Multimodal Learning

